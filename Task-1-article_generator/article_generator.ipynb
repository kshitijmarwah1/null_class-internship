import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
import gradio as gr

model_names = [
    "mistralai/Mistral-7B-Instruct-v0.2",
    "microsoft/Phi-3-mini-4k-instruct",
    "google/gemma-2b-it"
]

loaded_models = {}

print("Loading models... This may take a few minutes.")
for model_name in model_names:
    print(f"Loading {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True # Required for Phi-3
    )
    loaded_models[model_name] = (model, tokenizer)
    print(f"{model_name} loaded successfully.")

def generate_article(model_name, prompt):
    """Generates an article using the selected model."""
    model, tokenizer = loaded_models[model_name]

    # Use a pipeline for easier text generation
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.95
    )
    
    # For instruction-tuned models, it's best to format the prompt
    messages = [{"role": "user", "content": prompt}]
    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    outputs = pipe(formatted_prompt)
    article = outputs[0]["generated_text"]
    
    # Clean up the output to only show the generated part
    return article.replace(formatted_prompt, "").strip()

def chat_interface(prompt, model_choice):
    """Function to be called by the Gradio interface."""
    print(f"Generating with model: {model_choice} for prompt: '{prompt}'")
    return generate_article(model_choice, prompt)

model_dropdown = [
    "mistralai/Mistral-7B-Instruct-v0.2",
    "microsoft/Phi-3-mini-4k-instruct",
    "google/gemma-2b-it"
]

iface = gr.Interface(
    fn=chat_interface,
    inputs=[
        gr.Textbox(lines=3, label="Enter Article Topic", placeholder="e.g., The Future of Renewable Energy"), 
        gr.Radio(model_dropdown, label="Select LLM")
    ],
    outputs=gr.Markdown(label="Generated Article"),
    title="Article Generator Chatbot",
    description="Select a model and enter a topic to generate an article. This interface compares three different open-source LLMs."
)


print("\nLaunching Gradio Interface...")
iface.launch()


# This qualitative analysis compares three LLMs on their ability to generate an article about GenAI's impact on software development.

# ## Model Comparison

# * **mistralai/Mistral-7B-Instruct-v0.2:** **Outstanding.** Creates highly detailed, insightful, and exceptionally well-structured articles requiring minimal edits.
# * **microsoft/Phi-3-mini-4k-instruct:** **Excellent.** Produces coherent, relevant, and well-organized articles suitable for direct use. A highly efficient alternative.
# * **google/gemma-2b-it:** **Good.** Generates relevant content but is less polished and may require editing for structure.