import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
import gradio as gr
from sklearn.metrics import classification_report, confusion_matrix

# Model 1: LLaMA-2 (via Ollama, requires local setup)
# Model 2: Mistral-7B via Hugging Face
# Model 3: Phi-3 Mini via Hugging Face

model_names = [
    "mistralai/Mistral-7B-Instruct-v0.2",
    "microsoft/Phi-3-mini-4k-instruct"
]

models = []
tokenizers = []
for model_name in model_names:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
    models.append(model)
    tokenizers.append(tokenizer)

def generate_article(model_idx, prompt):
    tokenizer = tokenizers[model_idx]
    model = models[model_idx]

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_length=512)
    article = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return article

def chat_interface(prompt, model_choice):
    model_idx = 0 if model_choice == "Mistral-7B" else 1
    return generate_article(model_idx, prompt)

iface = gr.Interface(
    fn=chat_interface,
    inputs=[gr.Textbox(label="Enter Article Topic"), gr.Radio(["Mistral-7B", "Phi-3 Mini"], label="Select Model")],
    outputs="text",
    title="Article Generator Chatbot"
)

iface.launch()

y_true = ["Technology is evolving rapidly."] * 10
y_pred = ["Technology is evolving rapidly."] * 8 + ["The world is changing."] * 2

print("Classification Report:")
print(classification_report(y_true, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred))